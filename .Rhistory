library(knitr)
library
kntir
library(knitr)
This is an R Markdown document. Markdown is a simple formatting syntax for authoring web pages (click the **Help** toolbar button for more details on using R Markdown).
When you click the **Knit HTML** button a web page will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
install.packages('knitr', dependencies = TRUE)
x <- 1:4
x
p <- x/sum(x)
p
temp <- rbind(x, p)
temp
rownames(temp) <- c("X", "Prob")
temp
mean(temp)
The exponential distribution can be simulated in R with rexp(n, lambda) where lambda is the rate parameter. The mean of exponential distribution is 1/lambda and the standard deviation is also also 1/lambda. Set lambda = 0.2 for all of the simulations. In this simulation, you will investigate the distribution of averages of 40 exponential(0.2)s. Note that you will need to do a thousand or so simulated averages of 40 exponentials.
Illustrate via simulation and associated explanatory text the properties of the distribution of the mean of 40 exponential(0.2)s. You should
Show where the distribution is centered at and compare it to the theoretical center of the distribution.
Show how variable it is and compare it to the theoretical variance of the distribution.
Show that the distribution is approximately normal.
Evaluate the coverage of the confidence interval for 1/lambda: $\bar X\pm 1.96 \frac{s}{\sqrt{n}}$ (This only needs to be done for the specific value of lambda).
First, generate the data: 1000 trials of 40 exponentially distributed random variables. The mean and standard deviation of the set of 40 variables for each trial is stored in dataset.
set.seed(100)
lambda = 0.2
n = 40
trials = 1000
data = matrix(rexp(n*trials, lambda), nrow = trials, ncol = n)
dataset = data.frame(means = apply(data, 1, mean), sds = apply(data, 1, sd))
The theoretical mean is 1/lambda = r 1/lambda while the mean over all trials is r mean(dataset$means). The trial means are distributed following the CLT, so they should be normally distributed around 5 with a standard deviation of $5/\sqrt{40} =$ r 5/sqrt(40). Experimentally, the standard deviation of the means is r sd(dataset$means). The distribution is shown below, on the left, and it is approximately normally distributed because of the CLT. On the right, I examine the distribution of the standard deviation of the 1000 trials of the 40 exponential random variables. Theoretically, the standard deviation is 1/lambda = 5 and because of the CLT, I expect the same normally distributed experiments around 5 with a standrad deviation of r 5/sqrt(40). From the experimental data, the stndard deviations are centered around r mean(dataset$sds) with a standard deviation of r sd(dataset$sds). The plot below demonstrates that they standard deviations are approximately normally distributed in agreement with the CLT. The standard deviation of the exponential distribution is much more variable than the mean.
library(ggplot2)
require(gridExtra)
m = mean(dataset$means)
s = sd(dataset$means)
p1 <- ggplot(dataset, aes(x=means)) + xlim(2,8)
p1 <- p1 + geom_histogram(aes(y=..density..), color="black", fill="white", binwidth = lambda)
p1 <- p1 + stat_function(fun = dnorm, args=list(mean = m, sd = s), color="blue")
p1 <- p1 + geom_vline(xintercept = c(m-s, m+s), color="magenta")
p1 <- p1 + geom_vline(xintercept = c(m))
m = mean(dataset$sds)
s = sd(dataset$sds)
p2 <- ggplot(dataset, aes(x=sds)) + xlim(2,8)
p2 <- p2 + geom_histogram(aes(y=..density..), color="black", fill="white", binwidth = lambda)
p2 <- p2 + stat_function(fun = dnorm, args=list(mean = m, sd = s), color="blue")
p2 <- p2 + geom_vline(xintercept = c(m-s, m+s), color="magenta")
p2 <- p2 + geom_vline(xintercept = c(m))
#pushViewport(viewport(layout = grid.layout(1, 2,heights=unit(0.25, "npc"))))
grid.arrange(p1, p2, ncol=2)
Next, I examine the convergence of the exponential distribution's mean by repeating 1000 trials of an increasing number of exponential random variables and plotting the confidence interval around the mean.
dataset = data.frame()
for (n in (1:20)*50) {
data = matrix(rexp(n*trials, lambda), nrow = trials, ncol = n)
means = apply(data, 1, mean)
sds = apply(data, 1, sd)
dataset = rbind(dataset, data.frame(means = means, sds = sds, size = n))
}
datamean = aggregate(. ~ size, dataset, mean)
datasd =   aggregate(. ~ size, dataset, sd)
dataBySize = cbind(datamean, datasd)
names(dataBySize) <- c("size", "mean1", "mean2", "null", "sd1", "sd2")
dataBySize$null <- NULL
The means are represented below. The points are the experimental means and the experimental CI in shown as black lines. The theoretical mean is shown in blue and the theoretical CI is shown in magenta.
p <- qplot(size, mean1, data=dataBySize, formula=y~x, main = "Convergence and CI on the mean") + ylim(4.75,5.25)
p <- p + geom_line(aes(x = size, y = 1/lambda), color = "blue")
p <- p + geom_line(aes(x = size, y = 1/lambda + qnorm(.975) * 1/lambda / size), color = "magenta")
p <- p + geom_line(aes(x = size, y = 1/lambda - qnorm(.975) * 1/lambda / size), color = "magenta")
p <- p + geom_line(aes(x = size, y = mean1 + qnorm(.975) * sd1 / sqrt(size)))
p <- p + geom_line(aes(x = size, y = mean1 - qnorm(.975) * sd1 / sqrt(size)))
p
The standard deviations are represented below. The points are the experimental sd and the experimental CI in shown as black lines. The theoretical sd is shown in blue and the theoretical CI is shown in magenta.
p <- qplot(size, mean2, data=dataBySize, formula=y~x, main = "Convergence and CI on the sd") + ylim(4.75,5.25)
p <- p + geom_line(aes(x = size, y = 1/lambda), color = "blue")
p <- p + geom_line(aes(x = size, y = 1/lambda + qnorm(.975) * 1/lambda / size), color = "magenta")
p <- p + geom_line(aes(x = size, y = 1/lambda - qnorm(.975) * 1/lambda / size), color = "magenta")
p <- p + geom_line(aes(x = size, y = mean1 + qnorm(.975) * sd2 / sqrt(size)))
p <- p + geom_line(aes(x = size, y = mean1 - qnorm(.975) * sd2 / sqrt(size)))
p
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
z <- x*w
mean(z)
mean(z)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mean(x*w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
x
y
fit.origin <- lm( y ~ x - 1 )
summary(fit.origin)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
m.x <- mean(x)
sd.x <- sd(x)
(x[1] - m.x)/sd.x
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm( y ~ x )
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
e <- resid(fit)
sqe <- e*e
res.var <- sum(sqe) / (length(e) - 2)
sqrt(res.var)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
exp <- fit$coefficients[1] + mean(wt) * fit$coefficients[2]
exp - 2 * 0.5591
exp
help mtcars
?mtcars
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
summary(fit)
2 * (fit$coefficients[2] - 2 * 0.5591)
attributes(fit)
w.c <- fit$residuals ^ 2
fit.c <- lm(mpg ~ 1, mtcars)
fit.c.res <- fit.c$residuals ^ 2
sum(fit.c.res)
sum(w.c) /sum(fit.c.res)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
data(mtcars)
attach(mtcars)
fit <- lm(mpg ~ wt, mtcars)
summary(fit)
fit[[1]][1] + 3 * fit[[1]][2]
#manipulate(myPlot(beta), beta = slider(0.4, .8, step = 0.02))
#manipulate(myPlot(beta), beta = slider(0.4, .8, step = 0.02))
#manipulate(myPlot(beta), beta = slider(0.4, .8, step = 0.02))
myPlot <- function(beta){
myPlot <- function(beta){
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
plot(
as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent",
ylab = "child"
)
abline(0, beta, lwd = 3)
points(0, 0, cex = 2, pch = 19)
mse <- mean( (y - beta * x)^2 )
title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}
#manipulate(myPlot(beta), beta = slider(0.4, .8, step = 0.02))
myPlot <- function(beta){
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
plot(
as.numeric(as.vector(freqData$parent)),
as.numeric(as.vector(freqData$child)),
pch = 21, col = "black", bg = "lightblue",
cex = .15 * freqData$freq,
xlab = "parent",
ylab = "child"
)
abline(0, beta, lwd = 3)
points(0, 0, cex = 2, pch = 19)
mse <- mean( (y - beta * x)^2 )
title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}
#manipulate(myPlot(beta), beta = slider(0.4, .8, step = 0.02))
library(kernlab)
data(spam)
head(spam)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ss <- testing[,grep('^IL', x = names(testing) )]
model1 <- train(ss, testing$diagnosis, method='glm')
model2 <- preProcess(ss, method='pca', thresh = 0.8, outcome = testing$diagnosis)
model2
author("DevDataProducts")
library(slidify)
setwd("C:/Users/d026014/Documents/DevDataProducts/Slidify")
author("DevDataProducts")
slidify("index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
